# -*- coding: utf-8 -*-
"""Project_1_Yair.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kaUROFp8DuEjkSFfNpaaOWYLoDuoZkxd

# Project #1 Detecting Patterns in Tabular Medical Data with MIMIC-III
You can read more about the dataset here: https://www.kaggle.com/datasets/saurabhshahane/in-hospital-mortality-prediction

Dr. Barak Or
"""

from google.colab import drive
drive.mount('/content/drive')

my_path="/content/drive/MyDrive/first_project/"
#MIMIC_data.csv

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVR, SVC
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import mean_squared_error,root_mean_squared_error, r2_score
import seaborn as sns
import warnings
from sklearn.exceptions import UndefinedMetricWarning

# Load the data
data = pd.read_csv(my_path+"MIMIC_data.csv")
cols = ['age', 'BMI', 'Blood sodium']
subset = data[cols]


# === Helper Functions ===
def section_title(title: str):
    print("\n" + "=" * 60)
    print(title)
    print("=" * 60)

def label_ax(ax, title, xlabel, ylabel):
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.grid(True)

data.shape

"""# Q1.a
What are the mean, median, mode, and standard deviation of the age, BMI, and Blood sodium columns in the dataset? Why are these statistics important for understanding the data?
"""

#  Q1.a

# descriptive statistics
mean_vals = subset.mean() # Mean
median_vals = subset.median() # Median
mode_vals = subset.mode().iloc[0] # Mode (returns a DataFrame, take first row)
std_vals = subset.std() # Standard Deviation

# Display all together
summary = pd.DataFrame({
    "mean": mean_vals,
    "median": median_vals,
    "mode": mode_vals,
    "std": std_vals
}).round(3)

summary

"""**Why are these statistics important for understanding the data?**

**Looking at descriptive statistics like mean, median, mode, and standard deviation helps us quickly understand the center, spread, and shape of each feature's distribution. This guides us in spotting skewed data, outliers, or low-variance features, which is crucial for informed preprocessing and effective modeling.**

# Q1.b
How do the distributions of age, BMI, and Blood sodium look in the dataset? What can we learn from these distributions about the patient population?
"""

# Configuration
n = len(cols)
fig, axs = plt.subplots(2, n, figsize=(5*n, 8))  # 2 rows: hist + spread, n cols
lower, higher = 17, 42
flag = False # flag for ZoomFeat

# Helper to focus on some features more closely
def zoomFeat(columns, feature_name, axs, i, flag):
  if flag and columns[i] == feature_name:
      axs[1, i].set_xticks(np.arange(lower, higher + 1, 2))
      axs[1, i].set_xlim(lower - 1, higher + 1)  # Add slight padding

#Main plots

for i, feat in enumerate(cols):
    # Histogram (Top row)
    subset[feat].plot(kind='hist', bins=20, ax=axs[0, i], title=feat, color='skyblue')
    currSubPlot = axs[0, i]
    currSubPlot.spines[['top', 'right']].set_visible(False)
    currSubPlot.set_xlabel("")
    currSubPlot.set_ylabel("Count")

     # Dot plot (Bottom row) – use jitter for vertical spread
    x = subset[feat]
    y = np.random.uniform(0.9, 1.1, size=len(x))  # Add vertical jitter around y=1
    currSubPlot = axs[1, i]
    currSubPlot.scatter(x, y, alpha=0.4, s=10)
    currSubPlot.set_title(f"{feat} – spread")
    currSubPlot.set_yticks([])
    currSubPlot.spines[['top', 'right', 'left']].set_visible(False)
    # Optional zoom for BMI feature set flag=True (#Configuration section)
    zoomFeat(cols, "BMI", axs, i, flag)

plt.tight_layout()
plt.show()

"""**What can we learn from these distributions about the patient population?**

**We can learn about the spread of features.**

**Most patients are spread around the age 65-90, with BMI around 24-32 meaning they are overweight to obese. with blood sodium around 140.**

## Optional: Advanced Visualization for Q1.b

Seaborn visualization helps to understand the relation between raw data.
"""

# uncomment one of your prefered tuples of features to visualize:

tup = ["age", "BMI"]
# tup = ["age", "Blood sodium"]
# tup = ["BMI", "Blood sodium"]

# advanced plot
sns.jointplot(data=subset, x=tup[0], y=tup[1], kind="kde")
sns.scatterplot(data=subset, x=tup[0], y=tup[1], hue=tup[0], palette="viridis")

"""# Q1.c
Use pandas and scikit-learn to drop rows with missing values in the 'BMI' and 'Blood sodium' columns, and then uses logistic regression, SVM, kNN, and decision tree to predict an 'outcome' based on the features 'age', 'BMI', and 'Blood sodium'. Ensure to split the data using train_test_split with a 20% test size and a random state of 42. Finally, fit the model, make predictions on the test set, and print a report of the best model (“classification_report”). Explain the result of the confusion matrix for the best model.
"""

# function section

# --- MODELS ---
def create_classifiers():
    return {
        "LogisticRegression": LogisticRegression(),
        "SVM": SVC(),
        "KNN": KNeighborsClassifier(),
        "DecisionTree": DecisionTreeClassifier()
    }

# --- CLEAN DATA ---
def clean_data(df, features):
    return df.dropna(subset=features)

# --- SPLIT ---
def split_data(df, features, target):
    return train_test_split(df[features], df[target], test_size=0.2, random_state=42)

# --- TRAIN ---
def train_model(model, X_train, y_train):
    model.fit(X_train, y_train)
    return model

# --- PREDICT ---
def predict_model(model, X_test):
    return model.predict(X_test)

# --- EVALUATE ---
def evaluate_classifier(model_name, y_true, y_pred):
    # Suppress warning like: "Precision is ill-defined..."
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", UndefinedMetricWarning)
    # Return metrics only (no print here)
    return {
        'model': model_name,
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0),
        'preds': y_pred,
        'confusion_matrix': confusion_matrix(y_true, y_pred),
        'report': classification_report(y_true, y_pred, zero_division=0)
    }

# --- RESULTS AND DISCUSSION ---
def summarize_metrics(summary):
    # Summarize metrics for all the models handled.
    keys_to_keep = [k for k in summary[0] if k not in ['preds', 'report','confusion_matrix']]
    df = pd.DataFrame([{k: m[k] for k in keys_to_keep} for m in summary])
    print("\n Model Performance Summary:\n")
    print(df.round(3))  # rounded for readability

def plot_cm(cm,model_name):
  # plots Confusion Matrix
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)
    label_ax(ax, title=f'Confusion Matrix: {model_name}', xlabel='Predicted', ylabel='Actual')
    plt.tight_layout()
    plt.show()

def explain_confusion_matrix(cm):
  #explains Confusion Matrix fields
    tn, fp, fn, tp = cm.ravel()
    print("\n Confusion Matrix Explanation:")
    print(f" TN: {tn} - Correctly predicted class 0")
    print(f" FP: {fp} - Predicted 1 but it was 0")
    print(f" FN: {fn} - Predicted 0 but it was 1")
    print(f" TP: {tp} - Correctly predicted class 1")

def report_best_model(summary, metric):
  # Prints best model's Classification Report with Confusion Matrix explained
    best = max(summary, key=lambda x: x[metric]) #Get best by given metric
    model_name = best['model']
    cm = best['confusion_matrix']
    print(f"\n Best Model: {model_name}\n")
    print(best['report']) # Classification Report
    plot_cm(cm,model_name)
    explain_confusion_matrix(cm)

## Main pipeline of Q1.c

# Preperation section
target = 'outcome'
results = {}
summary = []
models = create_classifiers() # create classifiers models
df_clean = clean_data(data[cols + [target]], ['BMI', 'Blood sodium']) # clean data
X_train, X_test, y_train, y_test = split_data(df_clean, cols, target) # Data Split

for name, model in models.items():
    trained = train_model(model, X_train, y_train)
    preds = predict_model(trained, X_test)
    results[name] = preds
    # Evaluate and store results
    metrics = evaluate_classifier(name,y_test, preds)
    summary.append(metrics)

summarize_metrics(summary)        # prints accuracy, precision, recall, f1
print("\n ----------------------------------------------------------------- \n")
report_best_model(summary,'accuracy')  # full report + confusion matrix for best model

"""Self note for further explaining results:

**Poor learning - always predicting class 0 on the target 'outcome'. Reasons:**

  **1. model may underfits.**

  **2. data may be imbalanced - mostly class 0.**

  **3. selected features may not be good enough.**

# Q1.d
Predict BMI based on age and Blood sodium with linear regression, SVM regressor, Decision tree regressor, and kNN refressor. Calculate RMSE, MSE, R-squared.
Split where 20% left for the test, random state=42.
"""

import seaborn as sns

# Preprocessing
# Drop missing values for selected features
df_clean =clean_data(data, ['age', 'Blood sodium', 'BMI'])
# Split the dataset
X_train, X_test, y_train, y_test = split_data(df_clean,['age', 'Blood sodium'],'BMI')

# Define regression models
models = {
    'Linear Regression': LinearRegression(),
    'SVM Regressor': SVR(),
    'Decision Tree': DecisionTreeRegressor(),
    'kNN Regressor': KNeighborsRegressor()
}

results = {} # Store evaluation results
fig, axs = plt.subplots(2, 2, figsize=(12, 10))
axs = axs.ravel()  # flatten to 1D for easy iteration

# Evaluate each model
for idx, (name, model) in enumerate(models.items()):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    # calculate evaluations
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    results[name] = {'MSE': mse, 'RMSE': rmse, 'R²': r2}

    # Plot actual vs predicted
    axs[idx].scatter(y_test.values.ravel(), np.array(y_pred).ravel(), alpha=0.7)
    axs[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    label_ax(axs[idx], f"{name}", "Actual BMI", "Predicted BMI")
    axs[idx].grid(True)

plt.tight_layout()
plt.show()

# Display evaluation summary
results_df = pd.DataFrame(results).T.round(3)
print(results_df)

"""# Q2.a

Demonstrate the application of Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction on the dataset focusing on BMI, Blood sodium, and Blood calcium to visualize the data in a reduced-dimensional space. Compare the visualization results of PCA and t-SNE.
"""

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import plotly.graph_objects as go
from typing import Optional

# === 1. Preprocessing ===
def preprocess_data(data: pd.DataFrame, feats: list) -> tuple:
    df_clean = data[feats].dropna()
    return df_clean.values, df_clean

# === 2. PCA + Reconstruction ===
def apply_pca(data: np.ndarray, n_components: int) -> tuple:
    pca = PCA(n_components=n_components)
    data_pca = pca.fit_transform(data)
    data_recon = pca.inverse_transform(data_pca)
    return data_pca, data_recon, pca


# === 2D Static Plot ===
def plot_2d_static(original: np.ndarray, recon: np.ndarray, feats: list, plot_feats: list):
    plt.figure(figsize=(8, 6))
    plt.scatter(original[:, plot_feats[0]], original[:, plot_feats[1]], alpha=0.6, label='Original')
    plt.scatter(recon[:, plot_feats[0]], recon[:, plot_feats[1]], color='red', alpha=0.6, label='Reconstructed')

    for i in range(len(original)):
        plt.plot(
            [original[i, plot_feats[0]], recon[i, plot_feats[0]]],
            [original[i, plot_feats[1]], recon[i, plot_feats[1]]],
            'k-', alpha=0.2
        )

    title = f'PCA Compression from {original.shape[1]}D to 2D'
    label_ax(plt.gca(), title=title, xlabel=feats[plot_feats[0]], ylabel=feats[plot_feats[1]])
    plt.legend()
    plt.grid(True)
    plt.show()



# === Internal Helper for Plotly 3D Trace ===
def _make_scatter3d_trace(x, y, z, color, name, mode='markers', opacity=0.7, size=3, showlegend=True):
    return go.Scatter3d(
        x=x, y=y, z=z,
        mode=mode,
        marker=dict(size=size, color=color, opacity=opacity) if mode == 'markers' else None,
        line=dict(color=color, width=0.8) if mode == 'lines' else None,
        name=name if showlegend else None,
        showlegend=showlegend
    )

# === 3D Interactive Plot ===
def plot_3d_interactive(original: np.ndarray, recon: np.ndarray, feats: list, order: Optional[list] = None):
    if order is None:
        order = list(range(3))  # [0, 1, 2] by default
    fig = go.Figure()
    # Add original and reconstructed
    fig.add_trace(_make_scatter3d_trace(
        original[:, order[0]], original[:, order[1]], original[:, order[2]],
        color='blue', name='Original'
    ))

    fig.add_trace(_make_scatter3d_trace(
        recon[:, order[0]], recon[:, order[1]], recon[:, order[2]],
        color='red', name='PCA Reconstructed'
    ))

    # Add connection lines
    for i in range(len(original)):
        fig.add_trace(_make_scatter3d_trace(
            [original[i, order[0]], recon[i, order[0]]],
            [original[i, order[1]], recon[i, order[1]]],
            [original[i, order[2]], recon[i, order[2]]],
            color='black', name='', mode='lines', showlegend=False
        ))

    fig.update_layout(
        title='PCA 3D Compression and Reconstruction (Interactive)',
        scene=dict(
            xaxis_title=feats[order[0]],
            yaxis_title=feats[order[1]],
            zaxis_title=feats[order[2]],
            aspectmode='cube'
        ),
        margin=dict(l=10, r=10, b=10, t=40),
        height=500,
        width=750,
        showlegend=True
    )

    fig.show()

# Main pipeline of q2.a
feats = ['BMI', 'Blood sodium', 'Blood calcium']
plot_feats = [0, 2]
feat3d_order = [0, 1, 2]
n_components = 2

arr_df, df_clean = preprocess_data(data, feats)
data_pca, data_recon, pca = apply_pca(arr_df, n_components)

plot_3d_interactive(arr_df, data_recon, feats, order=feat3d_order)
plot_2d_static(arr_df, data_recon, feats, plot_feats)

#Summarize PCA info
print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Principal component direction(s):")
print(pca.components_)

# === 1. CONFIGURATION ===
feats = ['BMI', 'Blood sodium', 'Blood calcium']
label_col = 'outcome'

# === 2. DATA PREPROCESSING ===
# Select features and label, and drop rows with any NaNs
df_sub = data[feats + [label_col]].dropna()
# Extract arrays
arr_df = df_sub[feats].values
labels = df_sub[label_col].values

# === 3. APPLY t-SNE ===
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
data_tsne = tsne.fit_transform(arr_df)
fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# PCA plot
axs[0].scatter(data_pca[:, 0], data_pca[:, 1], c=labels, cmap='coolwarm', alpha=0.6)
label_ax(axs[0], title='PCA (2D)', xlabel='PC1', ylabel='PC2')

# t-SNE plot
axs[1].scatter(data_tsne[:, 0], data_tsne[:, 1], c=labels, cmap='coolwarm', alpha=0.6)
label_ax(axs[1], title='t-SNE (2D)', xlabel='TSNE-1', ylabel='TSNE-2')

plt.tight_layout()
plt.show()

"""# Q2.b
Apply K-means clustering to the dataset to group patients based on age, BMI, diabetes, and heart rate. Cluster to 2,3,4,5, and 6 groups. What are Silhouette and Davies-Bouldin Score for each case?
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score

# === Configuration ===
features = ['age', 'BMI', 'diabetes', 'heart rate']
cluster_range = [2, 3, 4, 5, 6]
n_comp = 2
random_state = 42

# === Preprocessing ===
df_clustering = data[features].dropna()
X_scaled = StandardScaler().fit_transform(df_clustering)

# === Clustering ===
results = []
labels_list = []

for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    sil = silhouette_score(X_scaled, labels)
    db = davies_bouldin_score(X_scaled, labels)
    results.append({'Clusters': k, 'Silhouette Score': sil, 'Davies-Bouldin Score': db})
    labels_list.append((k, labels))

# Clustering Scores Table
section_title("Clustering Evaluation Scores")
results_df = pd.DataFrame(results)
print(results_df)

# Clustering Scores Plot
section_title("Clustering Metrics Plot")
fig_scores, ax_scores = plt.subplots(figsize=(10, 5))
ax_scores.plot(results_df['Clusters'], results_df['Silhouette Score'], marker='o', label='Silhouette Score')
ax_scores.plot(results_df['Clusters'], results_df['Davies-Bouldin Score'], marker='s', label='Davies-Bouldin Score')
label_ax(ax_scores, 'Clustering Evaluation Metrics', 'Number of Clusters (k)', 'Score')
ax_scores.legend()
plt.tight_layout()
plt.show()

"""**What is the meaning of Davies-Bouldin value>1?**

**Answer: clusters are not separated well. Inner spreads instead of far apart from others. Selected features may not allow good separation**

## Advanced Visualization for Q2.b

Visualization helps to understand the relation between clusters after dimension reduction.
"""

def plot_clusters_grid(data_2d, labels_list, title, kind="PCA"):
    fig, axs = plt.subplots(2, 3, figsize=(15, 8))
    fig.suptitle(title, fontsize=16, y=0.95)

    for i, (k, labels) in enumerate(labels_list):
        row, col = divmod(i, 3)
        ax = axs[row][col]
        ax.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)
        label_ax(ax, f'k = {k}', f'{kind}-1', f'{kind}-2')

    for i in range(6 - len(labels_list)): axs[1][-(i + 1)].axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.94])
    plt.show()

# === Dimensionality Reduction ===
data_pca = PCA(n_components=n_comp).fit_transform(X_scaled)
data_tsne = TSNE(n_components=n_comp, random_state=random_state, perplexity=30).fit_transform(X_scaled)

# PCA Visualization
section_title("PCA Cluster Visualizations")
plot_clusters_grid(data_pca, labels_list, "K-means Clusters in PCA 2D", kind="PCA")

# t-SNE Visualization
section_title("t-SNE Cluster Visualizations")
plot_clusters_grid(data_tsne, labels_list, "K-means Clusters in t-SNE 2D", kind="t-SNE")

"""Comparing the visualisation results:

t-SNE make some kind of clusters, diffrent from PCA results, meaning the data has nonlinear structure.

#Q3.a
Describe the steps involved in training a neural network, including forward propagation and backpropagation.

## Answer
before looping the training Epochs, we init the weights and preprocessing the data.

Update weights: after each Batch on a given Epoch, we adjust the model Weights by getting closer to the minimum of a Loss function. using an algo like GD which uses:

*   forward propagation - flow of data, chain of sequentially computation from input layer to the next, until reaching output prediction and loss calculation.
*   backpropagation - method which computes gradient of the loss function using the chain rule for each layer, Loss is computed from output then we step back until reaching input.

Using this computation per Batch we can update *ALL* Weights using Gradient Descent, doing it for a set of Epochs. Final Weights can be used to generalize- to understand future data.

# Q3.b
Explain the bias-variance trade-off in neural network performance. How does it affect model generalization?

## Answer
Bias-Variance trade-off helps generalize, reaching a balance between simplifying (underfit) to making a complex model (overfit) in our learning.
It can be done by:

1. Adapting model structure - Depth, Width, Regularization and Droupouts.
2. Improve Learning - early stopping and learning rate adatptions.
3. Data intervention - larger dataset, data augmentation and feature selection.

# Q3.c
Highlight the importance of data preprocessing, normalization, and splitting for training effective deep learning models.

## Answer
Preprocessing makes data clean and usable by handling errors, missing values, and inconsistencies.

Normalization scales features to balance their influence, helping the model learn real patterns.

Splitting separates training and testing data to check if the model generalizes well to unseen cases, avoiding overfitting.

# Q3.d+e

Relevant library from tensorflow
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

#1 Preprocessing
features = ['age', 'Blood sodium']
target = 'outcome'
n_epochs = 100
batch_train = 8
batch_test = 32

# Clean data
clean_feat = features + [target]
df_clean = clean_data(data[clean_feat], clean_feat)
X= df_clean[features]
y = df_clean[target]
print(df_clean.shape)


#2 Spit data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# separating data like it should: test is not learned.
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)

# Standarize data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


#3  Build the model
model = Sequential([
    Dense(3, activation='relu', use_bias=True, input_shape=(X_train.shape[1],)),
    Dense(3, activation='relu', use_bias=True),
    Dropout(0.05),
    Dense(3, activation='relu', use_bias=True),
    Dropout(0.05),
    Dense(1, activation='sigmoid', use_bias=True),

])
#4 Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_train)
test_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_test)
history = model.fit(train_dataset, validation_data=test_dataset, epochs=n_epochs)

def visualize_training_history(history):
    # Visualize training history
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    label_ax(plt.gca(), 'Model Accuracy', 'Epochs', 'Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    label_ax(plt.gca(), 'Model Loss', 'Epochs', 'Loss')
    plt.legend()
    plt.show()

#4 Visualize training history
visualize_training_history(history)

#5 Evaluate and further visualization
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
section_title(f"Test Accuracy: {test_acc:.4f}")

# Predict probabilities
y_pred_prob = model.predict(X_test)
# Convert probabilities to binary predictions
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

# Plot confusion matrix
section_title("Confusion Matrix and Model Summary")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

model.summary()

# Tailored DNN for in-hospital morality

from tensorflow.keras.metrics import AUC, Precision, Recall
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.utils import class_weight

#1 Preprocessing
features = ['BMI','age', 'Blood sodium', 'Blood calcium', 'heart rate','diabetes']
target = 'outcome'
n_epochs = 100
batch_train = 8
batch_test = 32

# Clean data
clean_feat = features + [target]
df_clean = clean_data(data[clean_feat], clean_feat)
X= df_clean[features]
y = df_clean[target]

#2 Spit data
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)


# Standarize data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# # Compute class weights
# class_weights = class_weight.compute_class_weight(
#     class_weight='balanced',
#     classes=np.unique(y_train),
#     y=y_train
# )
# class_weights = dict(enumerate(class_weights))

#3  Build the model
model = Sequential([
    Dense(6, activation='relu', use_bias=True, input_shape=(X_train.shape[1],)),
    Dense(3, activation='relu', use_bias=True),
    Dropout(0.1),
    Dense(3, activation='relu', use_bias=True),
    Dropout(0.1),
    Dense(1, activation='sigmoid'),

])
#4 Compile the model
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',AUC(), Precision(), Recall()])

early_stop = EarlyStopping(
    monitor='val_loss',         # watch validation loss
    patience=5,                 # wait 5 epochs before stopping
    restore_best_weights=False  # rollback to best epoch
)

# Train the model
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_train)
val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_test)
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_test)
history = model.fit(train_dataset,
                    validation_data=val_dataset,
                    epochs=n_epochs,
                    callbacks=[early_stop])

visualize_training_history(history)

#5 Evaluate and further visualization
test_metrics = model.evaluate(X_test, y_test, verbose=0)

section_title(f"Test Accuracy: {test_metrics[1]:.4f}")
print(f"Test AUC: {test_metrics[2]:.4f}")
print(f"Test Precision: {test_metrics[3]:.4f}")
print(f"Test Recall: {test_metrics[4]:.4f}")

# Predict probabilities
y_pred_prob = model.predict(X_test)
# Convert probabilities to binary predictions
y_pred = (y_pred_prob > 0.4).astype(int).flatten()

# Plot confusion matrix
section_title("Confusion Matrix and Model Summary")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

model.summary()

"""**Conclusion: Dataset is too small to effectivly train a DNN, its similar results to classic ML classification methods like LogisticRegression and SVM.Even when using earlyStopping and advanced methods and tuning, a denser structure and feature list, there is not a big improvement.**"""
